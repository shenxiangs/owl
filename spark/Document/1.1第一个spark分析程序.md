一、第一个spark分析程序
在开始本节之前，先启动spark ，并熟悉spark编程指南。

这里需要在maven文件中添加spark的依赖 - 请注意更新基于您已经安装的spark的版本依赖：


<dependency> <!-- Spark -->
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-core_2.10</artifactId>
    <version>1.1.0</version>
</dependency>

在我们开始前，我们需要两样东西：

Apache的访问日志文件：如果你有一个，比它更有趣的真实的数据，那么请忽略以下内容。
这是一个微不足道提供实验数据 https://github.com/shenxiangs/owl/blob/master/spark/data/acl.txt。
一个日志文件分析器类：见 https://github.com/shenxiangs/owl/blob/master/spark/src/main/java/com/databricks/logs/ApacheAccessLog.java。
该示例代码使用Apache的访问日志文件，因为这是一个众所周知的，常见的日志格式。这使得你容易在其他真实的数据进行重写或实验。


以下数据将统计计算：

1、平均，最小和响应的最大规模的内容从服务器返回。
2、响应码伯爵的返回。
3、已访问此服务器超过N次所有IPAddresses。
4、顶部端点所要求的数量。
在第一次运行代码前，你需要写一个spark的日志分析类：https://github.com/shenxiangs/owl/blob/master/spark/src/main/java/com/databricks/apps/logs/chapter1/LogAnalyzer.java。

这个简单的spark应用的主体是如下。第一步是把一个创建一个SparkContext，然后SparkContext可以从文本文件中加载数据作为RDD。最后，在退出函数之前，stop SparkContext。

public class LogAnalyzer {
  public static void main(String[] args) {
    // Create a Spark Context.
    SparkConf conf = new SparkConf().setAppName("Log Analyzer");
    JavaSparkContext sc = new JavaSparkContext(conf);

    // Load the text file into Spark.
    if (args.length == 0) {
      System.out.println("Must specify an access logs file.");
      System.exit(-1);
    }
    String logFile = args[0];
    JavaRDD<String> logLines = sc.textFile(logFile);

    // TODO: Insert code here for processing logs.

    sc.stop();
  }
}
这是一个统计日志行的RDD，使用map功能，每一行转换为一个ApacheAccessLog对象。该ApacheAccessLog RDD被缓存在内存中，它可以被多种变换和动作调用。

// Convert the text log lines to ApacheAccessLog objects and
// cache them since multiple transformations and actions
// will be called on the data.
JavaRDD<ApacheAccessLog> accessLogs =
    logLines.map(ApacheAccessLog::parseFromLogLine).cache();
这是非常有用的定义总和的reducer - 这个函数传入两个参数，并返回它们的和。

private static Function2<Long, Long, Long> SUM_REDUCER = (a, b) -> a + b;
接下来，让我们计算平均值，最小值和response返回的最大内容大小。一个map提取内容的大小，然后不同的动作（reduce，count，min，和max）输出各种统计数据。再次，调用cache以避免重复调用RDD。

// Calculate statistics based on the content size.
// Note how the contentSizes are cached as well since multiple actions
//   are called on that RDD.
JavaRDD<Long> contentSizes =
   accessLogs.map(ApacheAccessLog::getContentSize).cache();
System.out.println(String.format("Content Size Avg: %s, Min: %s, Max: %s",
    contentSizes.reduce(SUM_REDUCER) / contentSizes.count(),
    contentSizes.min(Comparator.naturalOrder()),
    contentSizes.max(Comparator.naturalOrder())));
为了计算响应码，通过键值对mapToPair和reduceByKey，我们称之为通知take(100)，而不是collect()收集响应代码计数的最终输出。调用之前要格外小心collect()上的RDD，因为所有这些数据将被发送到spark，并可能导致耗尽内存的空间。但是在这种情况下，响应码的数量有限，而且似乎是安全的-如果有在Apache访问日志或在分析器中的错误畸形线，可能是很多无效响应代码以引起。

// Compute Response Code to Count.
List<Tuple2<Integer, Long>> responseCodeToCount = accessLogs
        .mapToPair(log -> new Tuple2<>(log.getResponseCode(), 1L))
        .reduceByKey(SUM_REDUCER)
        .take(100);
System.out.println(String.format("Response code counts: %s", responseCodeToCount));

下面是计算已经访问该服务器的大于10次的IPAddress，我们称之为filter改造的探讨然后map只提取指定IPAddress并丢弃计数。同样，我们使用take(100)检索值。

List<String> ipAddresses =
    accessLogs.mapToPair(log -> new Tuple2<>(log.getIpAddress(), 1L))
        .reduceByKey(SUM_REDUCER)
        .filter(tuple -> tuple._2() > 10)
        .map(Tuple2::_1)
        .take(100);
System.out.println(String.format("IPAddresses > 10 times: %s", ipAddresses));
最后，让我们计算在该日志文件所要求的top。我们定义了一个内部类ValueComparator以帮助这一点。这个函数告诉我们，给出了两个元组，它的价值是排序。

private static class ValueComparator<K, V>
   implements Comparator<Tuple2<K, V>>, Serializable {
  private Comparator<V> comparator;

  public ValueComparator(Comparator<V> comparator) {
    this.comparator = comparator;
  }

  @Override
  public int compare(Tuple2<K, V> o1, Tuple2<K, V> o2) {
    return comparator.compare(o1._2(), o2._2());
  }
}
然后，我们可以使用ValueComparator与top操作根据端点多少次访问，以计算该服务器上访问的top。

List<Tuple2<String, Long>> topEndpoints = accessLogs
    .mapToPair(log -> new Tuple2<>(log.getEndpoint(), 1L))
    .reduceByKey(SUM_REDUCER)
    .top(10, new ValueComparator<>(Comparator.<Long>naturalOrder()));
System.out.println("Top Endpoints: " + topEndpoints);
这些代码段在LogAnalyzer.java。现在，我们已经走通代码，然后试着运行这个例子。


请参阅语言具体说明，自述文件构建和运行。
